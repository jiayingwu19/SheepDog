LLM-Augmented (RoBERTa backbone, real news: fine-grained labels set to all-0)
-------------Test Variant: Original -------------
All Acc.s:[0.7708333333333334, 0.7588383838383839, 0.7506313131313131, 0.7550505050505051, 0.7582070707070707, 0.7588383838383839, 0.7544191919191919, 0.7626262626262627, 0.7487373737373737, 0.7588383838383839]
All Prec.s:[0.7711484593837534, 0.7612439821846921, 0.7506413025975744, 0.7559137996122518, 0.7602989603405853, 0.7598741284709106, 0.7545104849937564, 0.7626329617631304, 0.7488659201837496, 0.7590035478189847]
All Rec.s:[0.7708333333333333, 0.7588383838383839, 0.7506313131313131, 0.755050505050505, 0.7582070707070707, 0.7588383838383839, 0.7544191919191918, 0.7626262626262625, 0.7487373737373737, 0.7588383838383839]
All F1.s:[0.7707667301751326, 0.7582819341024576, 0.7506288284202147, 0.7548437537396189, 0.7577203011962788, 0.7585978529903181, 0.7543971674457147, 0.7626247489079487, 0.7487049234433969, 0.7587999311184954]
All F1-R:[0.7746741154562383, 0.769879518072289, 0.7498416719442684, 0.761963190184049, 0.7685800604229608, 0.7509778357235983, 0.7520713830465264, 0.7620253164556962, 0.7515605493133584, 0.761845386533666]
All F1-F:[0.766859344894027, 0.746684350132626, 0.7514159848961611, 0.7477243172951886, 0.7468605419695967, 0.766217870257038, 0.756722951844903, 0.7632241813602014, 0.7458492975734354, 0.7557544757033249]
Average acc.: 0.7577020202020204 
Average Prec / Rec / F1 (macro): 0.7584133547349389, 0.7577020202020203, 0.7575366171539578 
Average F1 by class (Real / Fake): 0.7603419027152649, 0.7547313315926504 
LLM-Augmented (RoBERTa backbone, real news: fine-grained labels set to all-0)
-------------Test Variant: A -------------
All Acc.s:[0.7563131313131313, 0.7563131313131313, 0.7310606060606061, 0.75, 0.7493686868686869, 0.7323232323232324, 0.7462121212121212, 0.7462121212121212, 0.7430555555555556, 0.7367424242424242]
All Prec.s:[0.7563539999107092, 0.7590904695910013, 0.7320609709235302, 0.7502297794117647, 0.7504904276529134, 0.7355009696186168, 0.7469064748201439, 0.746477749334491, 0.7430559430414356, 0.7368062255320167]
All Rec.s:[0.7563131313131313, 0.7563131313131313, 0.731060606060606, 0.75, 0.7493686868686869, 0.7323232323232323, 0.7462121212121212, 0.7462121212121212, 0.7430555555555556, 0.7367424242424242]
All F1.s:[0.7563034186330888, 0.7556583219173156, 0.7307704584192483, 0.7499425947187142, 0.7490877794566936, 0.7314172004069437, 0.7460335692851092, 0.746143726229678, 0.7430554531486955, 0.7367246910681907]
All F1-R:[0.7578419071518193, 0.7683073229291717, 0.7219321148825064, 0.753731343283582, 0.7574832009773976, 0.7158176943699731, 0.7392996108949417, 0.741976893453145, 0.7432176656151419, 0.7345639719923615]
All F1-F:[0.7547649301143582, 0.7430093209054595, 0.7396088019559902, 0.7461538461538462, 0.7406923579359895, 0.7470167064439142, 0.7527675276752767, 0.750310559006211, 0.7428932406822489, 0.73888541014402]
Average acc.: 0.7447601010101009 
Average Prec / Rec / F1 (macro): 0.7456973009836623, 0.7447601010101008, 0.7445137213283678 
Average F1 by class (Real / Fake): 0.7434171725550042, 0.7456102701017313 
LLM-Augmented (RoBERTa backbone, real news: fine-grained labels set to all-0)
-------------Test Variant: B -------------
All Acc.s:[0.7569444444444444, 0.7556818181818182, 0.7297979797979798, 0.7474747474747475, 0.7436868686868687, 0.7361111111111112, 0.7436868686868687, 0.7443181818181818, 0.7468434343434344, 0.7361111111111112]
All Prec.s:[0.7569776285390388, 0.7585210558130163, 0.7307175189351499, 0.7477844464308517, 0.7451894389015431, 0.7397824045591426, 0.7442491451216005, 0.7445244002648648, 0.7468532728353945, 0.7361653272101034]
All Rec.s:[0.7569444444444444, 0.7556818181818181, 0.7297979797979798, 0.7474747474747474, 0.7436868686868687, 0.7361111111111112, 0.7436868686868687, 0.7443181818181819, 0.7468434343434344, 0.7361111111111112]
All F1.s:[0.7569365976145637, 0.7550091580617921, 0.7295284845022413, 0.7473958167216315, 0.743293581541203, 0.7350971332916174, 0.7435392716257538, 0.7442642634502239, 0.7468409118898889, 0.7360959651035988]
All F1-R:[0.7583176396735718, 0.7678464307138573, 0.7209908735332463, 0.7518610421836228, 0.7533414337788579, 0.718707940780619, 0.7373868046571799, 0.7405509288917361, 0.7460417986067132, 0.7340966921119594]
All F1-F:[0.7555555555555556, 0.7421718854097269, 0.7380660954712362, 0.7429305912596402, 0.733245729303548, 0.751486325802616, 0.7496917385943278, 0.7479775980087118, 0.7476400251730647, 0.7380952380952382]
Average acc.: 0.7440656565656564 
Average Prec / Rec / F1 (macro): 0.7450764638610706, 0.7440656565656564, 0.7438001183802514 
Average F1 by class (Real / Fake): 0.7429141584931364, 0.7446860782673665 
LLM-Augmented (RoBERTa backbone, real news: fine-grained labels set to all-0)
-------------Test Variant: C -------------
All Acc.s:[0.7657828282828283, 0.7632575757575758, 0.7550505050505051, 0.764520202020202, 0.759469696969697, 0.75, 0.76010101010101, 0.764520202020202, 0.7556818181818182, 0.7588383838383839]
All Prec.s:[0.7660479159452253, 0.766938241070779, 0.7551302251111522, 0.7654550399564839, 0.7614541616666908, 0.7516432165964877, 0.760266979759565, 0.7645239974173184, 0.7558617025858974, 0.7590382590382591]
All Rec.s:[0.7657828282828283, 0.7632575757575758, 0.755050505050505, 0.764520202020202, 0.759469696969697, 0.75, 0.7601010101010102, 0.764520202020202, 0.7556818181818181, 0.7588383838383839]
All F1.s:[0.765724470794692, 0.7624386749837126, 0.7550313688288703, 0.7643127003762096, 0.7590124166681641, 0.749591211037302, 0.7600627587042623, 0.7645193573496232, 0.7556388684592893, 0.7587918544893271]
All F1-R:[0.7694220012430081, 0.776386404293381, 0.7528662420382165, 0.7713059472716125, 0.7695099818511798, 0.7394736842105262, 0.7570332480818414, 0.7649653434152489, 0.7588785046728972, 0.7621419676214198]
All F1-F:[0.7620269403463759, 0.7484909456740443, 0.7571964956195243, 0.7573194534808066, 0.7485148514851484, 0.7597087378640778, 0.7630922693266833, 0.7640733712839975, 0.7523992322456814, 0.7554417413572342]
Average acc.: 0.7597222222222223 
Average Prec / Rec / F1 (macro): 0.7606359739147858, 0.7597222222222223, 0.7595123681691454 
Average F1 by class (Real / Fake): 0.7621983324699332, 0.7568264038683573 
LLM-Augmented (RoBERTa backbone, real news: fine-grained labels set to all-0)
-------------Test Variant: D -------------
All Acc.s:[0.7664141414141414, 0.7626262626262627, 0.7537878787878788, 0.7619949494949495, 0.7537878787878788, 0.7537878787878788, 0.7575757575757576, 0.7626262626262627, 0.759469696969697, 0.7582070707070707]
All Prec.s:[0.7666590073529411, 0.7663786245124735, 0.7538461538461538, 0.7630858558934979, 0.7562735439003188, 0.7557860030464487, 0.7576809227314484, 0.7626413361648396, 0.7595628023137317, 0.7584250121662266]
All Rec.s:[0.7664141414141414, 0.7626262626262627, 0.7537878787878788, 0.7619949494949495, 0.7537878787878788, 0.7537878787878788, 0.7575757575757576, 0.7626262626262627, 0.759469696969697, 0.7582070707070707]
All F1.s:[0.7663605051664754, 0.7617873627120815, 0.7537737473454629, 0.7617479667498519, 0.7531894084312569, 0.753306103932822, 0.7575510204081632, 0.7626228567329076, 0.7594481254416898, 0.7581560812381131]
All F1-R:[0.7699004975124378, 0.7759237187127533, 0.7519083969465649, 0.7694189602446484, 0.7653429602888087, 0.7424042272126815, 0.7551020408163266, 0.7635220125786163, 0.7617260787992496, 0.761667703795893]
All F1-F:[0.7628205128205129, 0.7476510067114095, 0.755639097744361, 0.7540769732550555, 0.7410358565737052, 0.7642079806529626, 0.76, 0.7617237008871991, 0.7571701720841301, 0.7546444586803331]
Average acc.: 0.7590277777777779 
Average Prec / Rec / F1 (macro): 0.760033926192808, 0.7590277777777779, 0.7587943178158824 
Average F1 by class (Real / Fake): 0.7616916596907979, 0.7558969759409668 
